import torch
import torch.nn as nn

class HybridTransformerConvLSTM(nn.Module):
    def __init__(self, input_size, d_model, nhead, num_layers, conv_filters, hidden_size):
        super(HybridTransformerConvLSTM, self).__init__()

        # üìå –ò—Å–ø–æ–ª—å–∑—É–µ–º conv_filters –∫–∞–∫ input_size –¥–ª—è LSTM
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=conv_filters, kernel_size=3, padding=1)
        self.lstm = nn.LSTM(input_size=conv_filters, hidden_size=hidden_size, num_layers=2, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è

    def forward(self, x):
        x = self.conv1(x)
        x = x.permute(0, 2, 1)  # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –¥–ª—è LSTM
        x, _ = self.lstm(x)
        x = self.fc(x[:, -1, :])  # –í—ã–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–≥
        return x

    def to_device(self):
        """–ü–µ—Ä–µ–º–µ—â–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ"""
        self.to(self.device)
